---
title: "Module 4 Lab Submission"
author: "Ben Tankus"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(broom)
library(ggplot2)
library(MASS)
library(Sleuth3)
```

First, we will explore the Brain size data in the data set `case0902` from the `Sleuth3` library. You can read more about this data set by viewing the help file:

```{r echo=TRUE}
help(case0902)
head(case0902)
dfSpecies <- case0902
cor(dfSpecies[,-1])
```

1. **Fit a linear model with `Brain` as the response variable, and `Body`, `Gestation`, and `Litter` as the predictor variables.**

```{r echo=TRUE}

fit_full <- lm(Brain ~ Body + Gestation + Litter , data = case0902)
summary(fit_full)

```

2. **Calculate the case influence measures for this model using the `augment()` function from the package `broom`.  Which species has the highest leverage for this model? Which species has the highest Cook's Distance?**

```{r echo=TRUE}

augFit <- augment(fit_full)
augFit$species <- case0902$Species
head(augFit)

max_lev <- max(augFit$.hat)

augFit[augFit$.hat == max_lev, ]

augFit[augFit$.cooksd == max(augFit$.cooksd), ]

```

## Elephants have the highest leverage at *0.72*, and also the highest cook's distance at *24.29*



Now we will continue investigating multicollinearity. Recall the simulated scenario considered in the `M4Lab-examples.Rmd` file, where we followed these steps:

1. Define $\beta_{0} = 0.5$, $\beta_{1} = 0.3$, and $\beta_{2} = 0.7$
2. Define the mean of $X_{1}$ and $X_{2}$
3. Generate correlated/uncorrelated $X_{1}$ and $X_{2}$ data
4. Generate the response variable; use model equation and add N(0,1) noise
5. Fit a MLR model
6. Extract the coefficient estimate; $\hat{\beta}_{0}$, $\hat{\beta}_{1}$, or $\hat{\beta}_{2}$
7. Repeat steps (4) through (6) many times.

We used a function, included here, to perform steps 4. through 6., and then repeated that function many times (step 7.)

```{r, echo=TRUE}
fitmodel <- function(X1, X2, beta0, beta1, beta2){
  n <- length(X1)
  Y <- beta0 + beta1*X1 + beta2*X2 + rnorm(n, 0, 1) # Generate/calculate response 
  fit <- lm(Y ~ X1 + X2) # Fit the model
  fit$coefficients # Return estimated coefficient values
}
```

To run this function, we have to define the coefficient values (Step 1.), and set the mean and covariance matrix to generate predictor variables (Steps 2. and 3.).  

```{r, echo=TRUE, message=FALSE, fig.height= 4}
# Step 1
beta0 <- 0.5 # define beta_0
beta1 <- 0.3 # define beta_1, 
beta2 <- 0.7 # define beta_2

# Step 2
mu <- matrix(c(0,0)) # Set means for X_1, X_2
sigma1 <- matrix(c(1, 0, 0, 1), ncol = 2) # Cov Matrix: Cov(X_1, X_2) = 0
sigma2 <- matrix(c(1, 0.9, 0.9, 1), ncol = 2) # Cov Matrix: Cov(X_1, X_2) = 0


# Step 3
set.seed(1822) # Francis Galton born, invented regression concept

#UNCORRELATED
n <- 250
X <- mvrnorm(n, mu=c(0,0), Sigma=sigma1)
X1 <- X[,1]
X2 <- X[,2]

#CORRELATED
X2_2 <- mvrnorm(n, mu=c(0,0), Sigma=sigma2)
X1_2 <- X2_2[,1]
X2_2 <- X2_2[,2]


# Step 7
beta_estimates <- replicate(10000, fitmodel(X1, X2, beta0, beta1, beta2))
beta_estimatesCOV <- replicate(10000, fitmodel(X1_2, X2_2, beta0, beta1, beta2))


```

Finally, we calculated the standard deviation of the estimates of $\beta_0$ that resulted from these simulated datasets:

```{r, echo=TRUE}
print('uncorrelated')
print(paste("Intercept:", sd(beta_estimates[1,])))
print(paste("X1:", sd(beta_estimates[2,])))
print(paste("X2:", sd(beta_estimates[3,])))

print('')
print('correlated')
print(paste("Intercept:", sd(beta_estimatesCOV[1,])))
print(paste("X1:", sd(beta_estimatesCOV[2,])))
print(paste("X2:", sd(beta_estimatesCOV[3,])))

```

3. **Now it is your turn to calculate the standard deviation of the estimates of  $\beta_{1}$ and $\beta_{2}$ in the uncorrelated case; and $\beta_{0}$, $\beta_{1}$, and $\beta_{2}$ in the correlated case. As you run the simulations, fill in the standard errors in the table below. Note: In the correlated case, use `sigma2 <- matrix(c(1, 0.9, 0.9, 1), ncol = 2)` to define the covariance matrix.**

| **Parameter**  | **$SE(\hat\beta_i)$** |
|:--------------:|:---------------------:|
| *Uncorrelated* |                       |
|   $\beta_0$    |          0.0635       |
|   $\beta_1$    |          0.0614       |
|   $\beta_2$    |          0.062        |
|  *Correlated*  |                       |
|   $\beta_0$    |          0.0625       |
|   $\beta_1$    |          0.141        |
|   $\beta_2$    |          0.142        |




4. **The variances (and therefore standard deviations) of $\hat{\beta}_{1}$ and $\hat{\beta}_{2}$ are much larger when $X_1$ and $X_2$ are correlated than when they are uncorrelated. Does it make sense that $\hat{\beta}_{0}$ is unaffected? Explain your reasoning.**


## $\hat{\beta}_{0}$ being unaffected by correlation is not a suprise. This occurs based on the mathematical equation for standard error. The two terms on the right simplify to 1, leaving only SE = $\hat\sigma$ for both the uncorrelated and correlated models.


$$SE  =   \hat\sigma * \sqrt{\frac{1}{1-R^2_{x_j}}}\sqrt{\frac{1}{\sum(X_ij - \bar{X_j})^2}}   $$

5. **Recall the sample VIFs calculated (in `M4Lab-examples.Rmd`) for some simulated data in the correlated case:**

```{r}

speciesData <- case0902
#print("VIF")
#vif(fit_full)
print("")
print("Correlation Matrix")
cor(speciesData[,2:5])
```

X1: 5.304 , X2: 5.304

**Compare the variances (*squared standard deviations*) in the table above for the correlated predictor setting to the variances for the uncorrelated predictor setting: what is the ratio of the variance of $\hat{\beta}_1$ in the correlated predictor setting to the variance of $\hat{\beta}_1$ in the uncorrelated predictor setting?  Similarly, what is the variance of $\hat{\beta}_2$ in the correlated predictor setting to the variance of $\hat{\beta}_2$ in the uncorrelated predictor setting?  Do these ratios seem close to the VIFs that we calculated?**

```{r}

print(paste("Beta1 Corr / Uncorr:", round((0.141)^2 / (0.0614)^2,4) ))

print(paste("Beta2 Corr / Uncorr:", round((0.142)^2 / (0.062)^2,4) ))

#mean(c(1.82, 2.76, 1.74))

```

It seems that the beta 1 and beta 2 ratios are very close to the X1 and X2.
